{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Sequence to Sequence Model\n",
    "\n",
    "- [PyTorch implementation](https://github.com/bentrevett/pytorch-seq2seq/blob/master/5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_de,\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True,\n",
    "            batch_first=True)\n",
    "\n",
    "TRG = Field(tokenize=tokenize_en,\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, 'Kernel size must be odd!'\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        # 100は何？seq_lenの最大長か？\n",
    "        # 100個の単語をembeddingするのと同じように100種類の異なる長さをembeddingする\n",
    "        self.pos_embedding = nn.Embedding(100, emb_dim)\n",
    "\n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        # channel数 = featuresは2倍になる\n",
    "        # paddingがSAMEなのでseq_lenは変わらない\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=hid_dim,\n",
    "                                              out_channels=2 * hid_dim,\n",
    "                                              kernel_size=kernel_size,\n",
    "                                              padding=(kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [batch_size, seq_len]\n",
    "        \n",
    "        # create position tensor\n",
    "        pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(src.shape[0], 1).to(self.device)\n",
    "        \n",
    "        # pos = [batch_size, seq_len]\n",
    "        \n",
    "        # embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        # tok_embedded = pos_embedded = [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        # combine embeddings by elementwise summing\n",
    "        # concatではなくsumを使うのはなぜか？\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "\n",
    "        # embedded = [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        # pass embedded through linear layer to go through emb_dim => hid_dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        # conv_input = [batch_size, seq_len, hid_dim]\n",
    "        \n",
    "        # permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1)\n",
    "        \n",
    "        # conv_input = [batch_size, hid_dim, seq_len]\n",
    "        # Conv1dの入力は (N, C, L)\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # path through convolutional layer\n",
    "            conved = conv(self.dropout(conv_input))\n",
    "            \n",
    "            # conved = [batch_size, 2 * hid_dim, seq_len]\n",
    "            \n",
    "            # pass through GLU activation function\n",
    "            # TODO: hid_dimが半分になるのか？？？\n",
    "            conved = F.glu(conved, dim=1)\n",
    "            \n",
    "            # conved = [batch_size, hid_dim, seq_len]\n",
    "            \n",
    "            # apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            \n",
    "            # conved = [batch_size, hid_dim, seq_len]\n",
    "            \n",
    "            # set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "        \n",
    "        # permute and convert back to emb_dim\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        # conved = [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        # elementwise sum output (conved) and input (embedded) to be used for attention\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        \n",
    "        # combined = [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 5).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.arange(0, 5).unsqueeze(0).repeat(16, 1)\n",
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding = nn.Embedding(5, 256)\n",
    "pos_embedding(pos).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        self.pad_idx = pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(100, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(hid_dim, 2 * hid_dim, kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        # embedded = [batch_size, trg_seq_len, emb_dim]\n",
    "        # conved = [batch_size, hid_dim, trg_seq_len]\n",
    "        # encoder_conved = encoder_combined = [batch_size, src_seq_len, emb_dim]\n",
    "        \n",
    "        # permute and convert back to emb_dim\n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        combined = (embedded + conved_emb) * self.scale\n",
    "        \n",
    "        # combined = [batch_size, trg_seq_len, emb_dim]\n",
    "        \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "        \n",
    "        # energy = [batch_size, trg_seq_len, src_seq_len]\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        \n",
    "        # attention = [batch_size, trg_seq_len, src_seq_len]\n",
    "        \n",
    "        attended_encoding = torch.matmul(attention, (encoder_conved + encoder_combined))\n",
    "        \n",
    "        # attended_encoding = [batch_size, trg_seq_len, emb_dim]\n",
    "        \n",
    "        # convert from emb_dim => hid_dim\n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        \n",
    "        # attended_encoding = [batch_size, trg_seq_len, hid_dim]\n",
    "        \n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "        \n",
    "        # attended_combined = [batch_size, hid_dim, trg_seq_len]\n",
    "        \n",
    "        return attention, attended_combined\n",
    "    \n",
    "    def forward(self, trg, encoder_conved, encoder_combined):\n",
    "        # trg = [batch_size, seq_len]\n",
    "        # encoder_conved = encoder_combined = [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        # create position tensor\n",
    "        pos = torch.arange(0, trg.shape[1]).unsqueeze(0).repeat(trg.shape[0], 1).to(self.device)\n",
    "        \n",
    "        # pos = [batch_size, seq_len]\n",
    "        \n",
    "        # embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        # tok_embedded = pos_embedded = [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        # combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        # embedded = [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        # pass embedded through linear layer to go through emb_dim => hid_dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        # conv_input = [batch_size, seq_len, hid_dim]\n",
    "        \n",
    "        # permute for convokutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1)\n",
    "        \n",
    "        # conv_input = [batch_size, hid_dim, seq_len]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            conv_input = self.dropout(conv_input)\n",
    "            \n",
    "            # need to pad so decoder can't cheat\n",
    "            padding = torch.zeros(conv_input.shape[0], conv_input.shape[1],\n",
    "                                  self.kernel_size - 1).fill_(self.pad_idx).to(device)\n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim=2)\n",
    "            \n",
    "            # padded_conv_input = [batch_size, hid_dim, seq_len + kernel_size - 1]\n",
    "            \n",
    "            # pass through convolutional layer\n",
    "            conved = conv(padded_conv_input)\n",
    "            \n",
    "            # conved = [batch_size, 2 * hid_dim, seq_len]\n",
    "            \n",
    "            # pass through GLU activation function\n",
    "            conved = F.glu(conved, dim=1)\n",
    "            \n",
    "            # conved = [batch_size, hid_dim, seq_len]\n",
    "            \n",
    "            attention, conved = self.calculate_attention(embedded, conved,\n",
    "                                                         encoder_conved, encoder_combined)\n",
    "            \n",
    "            # attention = [batch_size, trg_seq_len, src_seq_len]\n",
    "            # conved = [batch_size, hid_dim, seq_len]\n",
    "            \n",
    "            # apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            \n",
    "            # conved = [batch_size, hid_dim, seq_len]\n",
    "            \n",
    "            # set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        # conved = [batch_size, seq_len, hid_dim]\n",
    "        \n",
    "        output = self.out(self.dropout(conved))\n",
    "        \n",
    "        # output = [batch_size, seq_len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        # src = [batch_size, src_seq_len]\n",
    "        # trg = [batch_size, trg_seq_len]\n",
    "        \n",
    "        # encoder_convedは、EncoderのConvブロックの出力\n",
    "        # encoder_combinedは、encoder_conved + src_embeddings + positional embeddings\n",
    "        encoder_conved, encoder_combined = self.encoder(src)\n",
    "        \n",
    "        # encoder_conved = encoder_combined = [batch_size, src_seq_len, emb_dim]\n",
    "        \n",
    "        # calculate predictions of next words\n",
    "        # outputはターゲットの単語の予測結果\n",
    "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "        \n",
    "        # output = [batch_size, trg_seq_len, output_dim]\n",
    "        # attention = [batch_size, trg_seq_len, src_seq_len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_LAYERS = 10\n",
    "DEC_LAYERS = 10\n",
    "ENC_KERNEL_SIZE = 3\n",
    "DEC_KERNEL_SIZE = 3\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, PAD_IDX, device)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(7855, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (emb2hid): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (hid2emb): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (8): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (9): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(5893, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (emb2hid): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (hid2emb): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (attn_hid2emb): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (attn_emb2hid): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (out): Linear(in_features=256, out_features=5893, bias=True)\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (8): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "      (9): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 37,351,685 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(src, trg[:, :-1])\n",
    "        \n",
    "        # output = [batch_size, trg_seq_len - 1, output_dim]\n",
    "        # trg = [batch_size, trg_seq_len]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        # output = [batch_size * trg_seq_len - 1, output_dim]\n",
    "        # trg = [batch_size * trg_seq_len - 1]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            output, _ = model(src, trg[:, :-1])\n",
    "            \n",
    "            # output = [batch_size, trg_seq_len - 1, output_dim]\n",
    "            # trg = [batch_size, trg_seq_len]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # output = [batch_size * trg_seq_len - 1, output_dim]\n",
    "            # trg = [batch_size * trg_seq_len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 32s\n",
      "\tTrain Loss: 4.352 | Train PPL:  77.635\n",
      "\tValid Loss: 3.086 | Valid PPL:  21.889\n",
      "Epoch: 02 | Time: 0m 32s\n",
      "\tTrain Loss: 3.019 | Train PPL:  20.469\n",
      "\tValid Loss: 2.419 | Valid PPL:  11.229\n",
      "Epoch: 03 | Time: 0m 32s\n",
      "\tTrain Loss: 2.594 | Train PPL:  13.382\n",
      "\tValid Loss: 2.170 | Valid PPL:   8.763\n",
      "Epoch: 04 | Time: 0m 32s\n",
      "\tTrain Loss: 2.363 | Train PPL:  10.620\n",
      "\tValid Loss: 2.011 | Valid PPL:   7.470\n",
      "Epoch: 05 | Time: 0m 32s\n",
      "\tTrain Loss: 2.207 | Train PPL:   9.092\n",
      "\tValid Loss: 1.922 | Valid PPL:   6.837\n",
      "Epoch: 06 | Time: 0m 32s\n",
      "\tTrain Loss: 2.091 | Train PPL:   8.094\n",
      "\tValid Loss: 1.901 | Valid PPL:   6.693\n",
      "Epoch: 07 | Time: 0m 32s\n",
      "\tTrain Loss: 2.003 | Train PPL:   7.409\n",
      "\tValid Loss: 1.848 | Valid PPL:   6.345\n",
      "Epoch: 08 | Time: 0m 32s\n",
      "\tTrain Loss: 1.935 | Train PPL:   6.921\n",
      "\tValid Loss: 1.809 | Valid PPL:   6.107\n",
      "Epoch: 09 | Time: 0m 32s\n",
      "\tTrain Loss: 1.871 | Train PPL:   6.498\n",
      "\tValid Loss: 1.794 | Valid PPL:   6.016\n",
      "Epoch: 10 | Time: 0m 32s\n",
      "\tTrain Loss: 1.824 | Train PPL:   6.199\n",
      "\tValid Loss: 1.762 | Valid PPL:   5.826\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 1.826 | Test PPL:   6.210 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut5-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
